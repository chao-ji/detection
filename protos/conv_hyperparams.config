type: CONV 

activation: RELU_6

regularizer {
  l2_regularizer {
    weight: 0.00004
  }
}

initializer {
  truncated_normal_initializer {
    stddev: 0.03
    mean: 0.0
  }
}

batch_norm {
  decay: 0.9997,
  epsilon: 0.001
  center: true,
  scale: true,
}

regularize_depthwise: true 

batch_norm_depthwise: true

activate_depthwise: true
